# 图像语义分割的深度学习算法回顾

---
![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216163833713-1192542209.jpg)

​                         用于东西分割的 COCO 数据集示例。来源： http ://cocodataset.org/

## 引言

深度学习算法已经解决了几个难度越来越大的计算机视觉任务。在我之前的博客文章中，我详细介绍了众所周知的：图像分类和对象检测。图像语义分割挑战包括将图像的每个像素（或仅几个像素）分类为一个实例，每个实例（或类别）对应于一个对象或图像的一部分（道路、天空……）。该任务是场景理解概念的一部分：深度学习模型如何更好地学习视觉内容的全局上下文？
物体检测任务在复杂度上已经超过了图像分类任务。它包括在图像中包含的对象周围创建边界框，并对每个对象进行分类。大多数对象检测模型使用锚框和提议来检测对象周围的边界框。不幸的是，只有少数模型考虑了图像的整个上下文，但它们只对一小部分信息进行分类。因此，它们无法提供对场景的全面理解。

为了理解一个场景，每个视觉信息都必须与一个实体相关联，同时考虑空间信息。要真正理解图像或视频中的动作，还出现了其他几个挑战：关键点检测、动作识别、视频字幕、视觉问答等。更好地理解环境将在许多领域有所帮助。例如，自动驾驶汽车需要以高精度划定路边才能自行移动。在机器人技术中，生产机器应该了解如何抓取、转动和组合两个需要界定物体确切形状的不同部件。
在这篇博文中，详细介绍了一些先前关于图像语义分割挑战的最先进模型的架构。请注意，研究人员使用不同的数据集（PASCAL VOC、PASCAL Context、COCO、Cityscapes）测试他们的算法，这些数据集在年份之间是不同的，并且使用不同的评估指标。因此，所引用的性能本身不能直接比较。此外，结果取决于预训练的顶级网络（主干网络），这篇文章中发布的结果对应于每篇论文中发布的关于其测试数据集的最佳分数。



## 数据集和指标

### **PASCAL 视觉对象类 (PASCAL VOC)**

PASCAL VOC 数据集 (2012) 是众所周知的常用于对象检测和分割的数据集。超过 11k 幅图像组成了训练和验证数据集，而 10k 幅图像专用于测试数据集。

分割挑战使用平均交叉联合 (mIoU)指标进行评估。Intersection over Union (IoU) 是一种也用于对象检测的度量，用于评估预测位置的相关性。IoU是ground truth和预测区域之间的重叠区域和联合区域之间的比率。mIoU 是分割对象在测试数据集的所有图像上的 IoU 之间的平均值。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216163955767-1729179833.png)

用于图像分割的 2012 PASCAL VOC 数据集示例。来源：http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html



### PASCAL-Context

PASCAL-Context 数据集 (2014) 是 2010 PASCAL VOC 数据集的扩展。它包含大约 10k 用于训练的图像，10k 用于验证和 10k 用于测试。这个新版本的特点是整个场景被分割提供了 400 多个类别。请注意，图像已由六名内部注释者在三个月内进行了注释。
PASCAL-Context 挑战的官方评估指标是 mIoU。其他几个指标由研究发布为像素精度 (pixAcc)。在这里，性能将仅与 mIoU 进行比较。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164016501-2142013157.png)

PASCAL-Context 数据集的示例。资料来源：https ://cs.stanford.edu/~roozbeh/pascal-context/



### COCO

图像语义分割（“物体检测”和“物体分割”）有两个 COCO 挑战（2017 年和 2018 年）。“对象检测”任务包括将对象分割和分类为 80 个类别。“东西分割”任务使用图像的大部分分割部分（天空、墙壁、草）的数据，它们包含几乎所有的视觉信息。在这篇博文中，将只比较“对象检测”任务的结果，因为引用的研究论文中很少有关于“物体分割”任务的结果。
用于对象分割的 COCO 数据集由超过 200k 的图像和超过 500k 的对象实例分割组成。它包含一个训练数据集、一个验证数据集、一个用于研究人员的测试数据集（test-dev）和一个用于挑战的测试数据集（test-challenge）。两个测试数据集的注释都不可用。这些数据集包含 80 个类别，并且仅分割了相应的对象。此挑战使用与对象检测挑战相同的指标：平均精度 (AP) 和平均召回率 (AR) 均使用联合交集 (IoU)。
有关 IoU 和 AP 指标的详细信息，请参阅我之前的博客文章。例如 AP，Average Recall 是使用具有特定重叠值范围的多个 IoU 计算的。对于固定的 IoU，具有相应测试/地面实况重叠的对象被保留。然后为检测到的对象计算召回指标。最终的 AR 指标是所有 IoU 范围值的计算召回率的平均值。基本上，用于分割的 AP 和 AR 度量与对象检测的工作方式相同，除了 IoU 是按像素计算的，用于语义分割的非矩形形状。


![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164045108-1923764315.png)
用于对象分割的 COCO 数据集示例。来源：http ://cocodataset.org/

### Cityscapes

Cityscapes 数据集于 2016 年发布，包含来自 50 个城市的复杂分段城市场景。它由 23.5k 用于训练和验证的图像（精细和粗略注释）和 1.5 张用于测试的图像（仅精细注释）组成。图像是完全分割的，例如具有 29 个类别的 PASCAL-Context 数据集（在 8 个超类别内：平面、人类、车辆、建筑、物体、自然、天空、虚空）。由于其复杂性，它通常用于评估语义分割模型。它还因其与自动驾驶应用的真实城市场景的相似性而闻名。语义分割模型的性能是使用 mIoU 指标计算的，例如 PASCAL 数据集。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164107847-1422099759.png)

Cityscapes 数据集的示例。顶部：粗略的注释。底部：精细注释。来源：https ://www.cityscapes-dataset.com/

## 网络模型

### Fully Convolutional Network (FCN)

J. Long 等人。（2015 年）率先开发了一个完全卷积网络（FCN）（仅包含卷积层），用于图像分割的端到端训练。
FCN 获取任意大小的图像并生成相同大小的分割图像。作者首先修改了众所周知的架构（AlexNet、VGG16、GoogLeNet），使其具有非固定大小的输入，同时用卷积层替换所有全连接层。由于网络生成了几个具有小尺寸和密集表示的特征图，因此需要上采样来创建与输入尺寸相同的输出。基本上，它包含一个步幅小于 1 的卷积层。通常称为反卷积因为它创建了一个比输入更大的输出。这样，网络使用逐像素损失进行训练。此外，他们在网络中添加了跳跃连接，以将高级特征图表示与网络顶部更具体和密集的特征图表示相结合。
作者使用 2012 ImageNet 数据集上的预训练模型在 2012 PASCAL VOC 分割挑战中达到了 62.2% 的 mIoU 分数。对于 2012 PASCAL VOC 对象检测挑战，名为 Faster R-CNN 的基准模型已达到 78.8% mIoU。即使我们不能直接比较两个结果（不同的模型、不同的数据集和不同的挑战），看起来语义分割任务比目标检测任务更难解决。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164128743-1313685319.png)

FCN 的架构。请注意，此处未绘制跳过连接。来源：J. Long 等人。(2015)



### ParseNet

W.刘等人。(2015)发表了一篇论文，解释了J. Long 等人的 FCN 模型的改进。（2015 年）。根据作者的说法，FCN 模型通过专门生成的特征图在其深层丢失了图像的全局上下文。ParseNet 是一个端到端的卷积网络，同时预测所有像素的值，它避免将区域作为输入来保留全局信息。作者使用了一个以特征图作为输入的模块。第一步使用模型生成特征图，将其简化为具有池化层的单个全局特征向量。使用L2 Euclidian Norm对该上下文向量进行归一化并且它是非池化的（输出是输入的扩展版本）以生成与初始特征图大小相同的新特征图。第二步使用 L2 Euclidian Norm 对整个初始特征图进行归一化。最后一步连接前两个步骤生成的特征图。归一化有助于缩放连接的特征图值，并带来更好的性能。基本上，ParseNet 是一个 FCN，这个模块取代了卷积层。它在 PASCAL-Context 挑战中获得了 40.4% 的 mIoU 分数，在 2012 PASCAL VOC 分割挑战中获得了 69.8% 的 mIoU 分数。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164152144-1209777888.png)
FCN 和 ParseNet 的分割和 ParseNet 模块的架构比较。资料来源：W. Liu 等人。(2015)



### Convolutional and Deconvolutional Networks

卷积和反卷积网络
H. Noh 等人。(2015)发布了一个由两个链接部分组成的端到端模型。第一部分是具有 VGG16 架构的卷积网络。它将实例提议作为输入，例如由对象检测模型生成的边界框。该提案由卷积网络处理和转换，以生成特征向量。第二部分是一个反卷积网络，将特征向量作为输入，并生成属于每个类的逐像素概率图。反卷积网络使用unpooling以最大激活为目标，以保持地图中信息的位置。第二个网络还使用反卷积将单个输入关联到多个特征图。反卷积扩展了特征图，同时保持信息密集。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164214542-1677629141.png)

卷积网络层（池化和卷积）与反卷积网络层（解池化和反卷积）的比较。资料来源：H. Noh 等人。(2015)

作者分析了反卷积特征图，他们注意到低层特征图是特定于形状的，而高层特征图有助于对提案进行分类。最后，当一张图像的所有proposals被整个网络处理后，将这些地图连接起来，得到完全分割的图像。该网络在 2012 PASCAL VOC 分割挑战中获得了 72.5% 的 mIoU。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164233180-880365882.png)

全网架构。卷积网络基于 VGG16 架构。反卷积网络使用反池化和反卷积层。资料来源：H. Noh 等人。(2015)

### U-Net

O. Ronneberger 等人。（2015）扩展了J. Long 等人的 FCN。(2015)用于生物显微镜图像。作者创建了一个称为 U-net 的网络，由两部分组成：一个用于计算特征的收缩部分和一个用于对图像中的模式进行空间定位的扩展部分。下采样或收缩部分具有类似 FCN 的架构，通过 3x3 卷积提取特征。上采样或扩展部分使用上卷积（或反卷积）减少特征图的数量，同时增加它们的高度和宽度。从网络的下采样部分裁剪的特征图被复制到上采样部分，以避免丢失模式信息。最后，1x1 卷积处理特征图以生成分割图，从而对输入图像的每个像素进行分类。从那时起，U-net 架构在最近的作品（FPN、PSPNet、DeepLabv3 等）中得到了广泛的扩展。请注意，它不使用任何全连接层。结果，模型的参数数量减少了，并且可以使用小的标记数据集（使用适当的数据增强）进行训练。例如，作者在实验期间使用了一个包含 30 张图像的公共数据集进行训练。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164253040-323294748.png)

给定输入图像的 U-net 架构。蓝色框对应于具有表示形状的特征图块。白框对应于复制和裁剪的特征图。资料来源：O. Ronneberger 等人。(2015)

### Feature Pyramid Network (FPN)

特征金字塔网络
特征金字塔网络 (FPN) 由 T.-Y 开发。Lin et al (2016)用于对象检测或图像分割框架。它的架构由自下而上的路径、自上而下的路径和横向连接组成，以连接低分辨率和高分辨率特征。自下而上的路径将任意大小的图像作为输入。它使用卷积层进行处理，并通过池化层进行下采样。请注意，每一束大小相同的特征图称为一个阶段，每个阶段的最后一层的输出是用于金字塔级别的特征。自上而下的路径包括使用 unpooling 对最后的特征图进行上采样，同时使用横向连接使用自下而上路径的同一阶段的特征图增强它们。这些连接包括将使用 1x1 卷积（以减小其维度）处理的自下而上路径的特征图与自上而下路径的特征图合并。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164323857-1859577281.png)

具有横向连接和特征图总和的自上而下块过程的详细信息。资料来源：T.-Y。林等人（2016）


然后通过 3x3 卷积处理连接的特征图以产生阶段的输出。最后，自上而下路径的每个阶段都会生成一个预测来检测物体。对于图像分割，作者使用两个多层感知器 (MLP) 在对象上生成两个不同大小的掩码。它的工作原理类似于带有锚框的区域建议网络 (R-CNN R. Girshick et al. (2014)、Fast R-CNN R. Girshick et al. (2015)、Faster R-CNN S. Ren et al. (2016 ) )等等)。这种方法是有效的，因为它可以更好地将低信息传播到网络中。基于 DeepMask ( P. 0. Pinheiro et al. (2015) ) 和 SharpMask ( P. 0. Pinheiro et al. (2016)的 FPN) 框架在 2016 年 COCO 分割挑战中获得了 48.1% 的平均召回率 (AR) 分数。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164343367-1913255384.png)

架构比较。(a)：图像以多种尺寸进行缩放，每个尺寸都经过卷积处理，以提供计算量大的预测。(b)：图像具有由 CNN 处理的单一尺度，具有卷积和池化层。© CNN 的每一步都用于提供预测。(d) FPN 的架构，左侧自下而上，右侧自上而下。资料来源：T.-Y。林等人（2016）



### Pyramid Scene Parsing Network (PSPNet)


H.赵等人。（2016）开发了金字塔场景解析网络（PSPNet），以更好地学习场景的全局上下文表示。使用具有扩张网络策略¹的特征提取器（ResNet K. He et al. (2015)）从输入图像中提取模式。特征图为金字塔池化模块提供数据以区分不同尺度的模式。它们用四种不同的尺度进行池化，每一种对应于一个金字塔级别，并由一个 1x1 卷积层处理以减小它们的尺寸。这样，每个金字塔级别都会分析图像中具有不同位置的子区域。金字塔级别的输出被上采样并连接到初始特征图，最终包含局部和全局上下文信息。然后，它们由卷积层处理以生成逐像素预测。具有预训练 ResNet（使用 COCO 数据集）的最佳 PSPNet 在 2012 PASCAL VOC 分割挑战中达到了 85.4% 的 mIoU 分数。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164411095-398576661.png)

PSPNet 架构。输入图像 (a) 由 CNN 处理以生成特征图 (b)。他们提供一个金字塔池化模块©，最后一个卷积层生成像素预测。资料来源：H. Zhao 等。(2016)

### Mask R-CNN

K. 他等人。(2017)发布了 Mask R-CNN 模型，在许多 COCO 挑战中击败了之前的所有基准²。我已经在我之前的博客文章中提供了有关用于对象检测的 Mask R-CNN 的详细信息。提醒一下，用于对象检测的 Faster R-CNN ( S. Ren et al. (2015) ) 架构使用区域提议网络(RPN) 来提议候选边界框。RPN 提取感兴趣区域(RoI) 和RoIPool层从这些提议中计算特征，以推断边界框坐标和对象的类别。Mask R-CNN 是一个 Faster R-CNN，具有 3 个输出分支：第一个计算边界框坐标，第二个计算关联类，最后一个计算二进制掩码³以分割对象。二进制掩码具有固定大小，由 FCN 针对给定的 RoI 生成。它还使用RoIAlign层而不是 RoIPool 来避免由于 RoI 坐标的量化而导致的错位。Mask R-CNN 模型的特殊性在于它的多任务损失结合边界框坐标、预测类别和分割掩码的损失。该模型试图解决互补的任务，从而在每个单独的任务上获得更好的表现。最好的 Mask R-CNN 使用 ResNeXt ( S. Xie et al. (2016) ) 来提取特征和 FPN 架构。它在 2016 年 COCO 分割挑战中获得了 37.1% 的 AP 分数，在 2017 年 COCO 分割挑战中获得了 41.8% 的 AP 分数。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164439663-756846196.png)
Mask R-CNN 架构。第一层是提取 RoI 的 RPN。第二层处理 RoI 以生成特征图。它们直接用于计算边界框坐标和预测类别。特征图也由 FCN（第三层）处理以生成二进制掩码。资料来源：K. He 等人。(2017)

### DeepLab、DeepLabv3 和 DeepLabv3+

#### DeepLab

灵感来自 T.-Y 的 FPN 模型。Lin 等人 (2016)，L.-C. 陈等人。(2017)发布了 DeepLab，结合了多孔卷积、空间金字塔池和完全连接的 CRF。本文介绍的模型也称为 DeepLabv2，因为它是对初始 DeepLab 模型的调整（为了避免冗余，将不提供有关初始模型的详细信息）。根据作者的说法，连续的最大池化和跨步降低了深度神经网络中特征图的分辨率。他们引入了空洞卷积，基本上是H. Zhao 等人的扩张卷积。(2016). 它由以固定速率针对稀疏像素的过滤器组成。例如，如果比率等于 2，则过滤器以输入中的二分之一像素为目标；如果比率等于 1，则 atrous 卷积是基本卷积。Atrous 卷积允许捕获多个尺度的对象。当它在没有最大池化的情况下使用时，它会在不增加权重数量的情况下增加最终输出的分辨率。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164503419-805025915.png)

低分辨率输入上的标准卷积（顶部）和高分辨率输入（底部）上速率为 2 的空洞卷积之间的提取模式比较。资料来源：L.-C。陈等人。(2017)


Atrous Spatial Pyramid Pooling包括应用多个具有不同速率的相同输入的 atrous 卷积来检测空间模式。特征图在单独的分支中处理，并使用双线性插值连接以恢复输入的原始大小。输出提供一个完全连接的条件随机场 (CRF) ( Krähenbühl 和 V. Koltun (2012) )，计算特征和长期依赖关系之间的边缘以产生语义分割。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164522770-1271775537.png)

Atrous Spatial Pyramid Pooling (ASPP) 利用多尺度对象对中心像素进行分类。资料来源：L.-C。陈等人。(2017)

使用 ResNet-101 作为主干的最佳 DeepLab 在 2012 年 PASCAL VOC 挑战中的 mIoU 得分达到 79.7%，在 PASCAL-Context 挑战中的 mIoU 得分为 45.7%，在 Cityscapes 挑战中的 mIoU 得分为 70.4%。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164541063-2108065735.png)

DeepLab 框架。资料来源：L.-C。陈等人。(2017)

#### DeepLabv3

L.-C. 陈等人。（2017 年）重新审视了 DeepLab 框架，以创建 DeepLabv3，结合了多孔卷积的级联和并行模块。作者修改了 ResNet 架构，以使用 atrous 卷积将高分辨率特征图保留在深层块中。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164556284-1069210372.png)

ResNet 架构中的级联模块。资料来源：L.-C。陈等人。(2017)

并行的空洞卷积模块在空洞空间金字塔池（ASPP）中分组。在 ASPP 中添加了 1x1 卷积和批量归一化。所有输出都由另一个 1x1 卷积连接和处理，以创建带有每个像素的 logits 的最终输出。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164610681-535484812.png)

Deeplabv3 框架中的 Atrous Spatial Pyramid Pooling。资料来源：L.-C。陈等人。(2017)

在 ImageNet 和 JFT-300M 数据集上预训练 ResNet-101 的最佳 DeepLabv3 模型在 2012 年 PASCAL VOC 挑战赛中达到了 86.9% 的 mIoU 分数。它还在 Cityscapes 挑战赛中获得了 81.3% 的 mIoU 分数，该模型仅使用相关的训练数据集进行了训练。



#### DeepLabv3+

L.-C. 陈等人。(2018)终于发布了使用编码器-解码器结构的 Deeplabv3+ 框架。作者介绍了由深度卷积（输入的每个通道的空间卷积）和点卷积（1x1 卷积，深度卷积作为输入）组成的空洞可分离卷积。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164642587-1314964045.png)

Depthwise 卷积 (a) 和 Pointwise 卷积 (b) 的组合以创建 Atrous Separable Convolution（速率为 2）。资料来源：L.-C。陈等人。(2018)


他们使用 DeepLabv3 框架作为编码器。性能最高的模型具有改进的 Xception ( F. Chollet (2017) ) 主干，具有更多层、多孔深度可分离卷积，而不是最大池化和批量归一化。ASPP 的输出由 1x1 卷积处理，并向上采样 4 倍。编码器主干 CNN 的输出也由另一个 1x1 卷积处理，并与之前的卷积连接。特征图提供两个 3x3 卷积层，输出被上采样 4 倍以创建最终的分割图像。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164704390-232461441.png)

DeepLabv3+ 框架：具有主干 CNN 和 ASPP 的编码器产生特征表示，以提供具有 3x3 卷积的解码器，从而产生最终的预测图像。资料来源：L.-C。陈等人。(2018)

在 COCO 和 JFT 数据集上预训练的最佳 DeepLabv3+ 在 2012 PASCAL VOC 挑战赛中获得了 89.0% 的 mIoU 分数。在 Cityscapes 数据集上训练的模型在相关挑战中达到了 82.1% 的 mIoU 分数。

### Path Aggregation Network (PANet)

路径聚合网络 (PANet)

S.刘等人。（2018）最近发布了路径聚合网络（PANet）。该网络基于 Mask R-CNN 和 FPN 框架，同时增强了信息传播。网络的特征提取器使用 FPN 架构，具有新的增强自下而上路径，改善了低层特征的传播。第三条路径的每个阶段都将前一阶段的特征图作为输入，并使用 3x3 卷积层对其进行处理。使用横向连接将输出添加到自上而下路径的同一阶段特征图，这些特征图为下一阶段提供信息。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164733669-1486212463.png)

自上而下路径和增强的自下而上路径之间的横向连接。资料来源：S. Liu 等人。(2018)


增强的自下而上路径的特征图与 RoIAlign 层池化，以从所有级别的特征中提取建议。自适应特征池化层使用全连接层处理每个阶段的特征图，并将所有输出连接起来。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164758891-234714260.png)

数据特征池化层。资料来源：S. Liu 等人。(2018)
自适应特征池化层的输出与 Mask R-CNN 类似，提供三个分支。前两个分支使用全连接层来生成边界框坐标和相关对象类的预测。第三个分支使用 FCN 处理 RoI，以预测检测到的对象的二进制像素级掩码。作者添加了一条路径，用于处理具有全连接层的 FCN 卷积层的输出，以改善预测像素的定位。最后，并行路径的输出被重新整形并连接到 FCN 的输出，生成二进制掩码。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216164813323-1723737266.png)

PANet 的分支使用 FCN 和具有全连接层的新路径预测二进制掩码。来源：https ://arxiv.org/pdf/1803.01534.pdf
PANet 使用 ResNeXt 作为特征提取器在 2016 年 COCO 分割挑战中取得了 42.0% 的 AP 分数。他们还使用七个特征提取器的集合以 46.7% 的 AP 分数执行了 2017 年 COCO 分割挑战：ResNet ( K. He et al. (2015)、ResNeXt ( S. Xie et al. (2016) ) 和 SENet ( J. . 胡等人（2017））。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216165013974-2089494117.png)

PANet 架构。(a)：使用 FPN 架构的特征提取器。(b)：新的增强自下而上的路径添加到 FPN 架构中。©：自适应特征池化层。(d)：预测边界框坐标和目标类别的两个分支。(e)：预测对象二进制掩码的分支。虚线对应于低级和高级模式之间的链接，红色的在 FPN 中，包含超过 100 层，绿色的是 PANet 中的捷径，由不到 10 层组成。资料来源：S. Liu 等人。(2018)

### Context Encoding Network (EncNet)

上下文编码网络 (EncNet)
H.张等人。(2018)创建了一个上下文编码网络 (EncNet)，用于捕获图像中的全局信息以改进场景分割。该模型首先使用基本特征提取器 (ResNet)，并将特征图输入到上下文编码模块中，该模块受H. Zhang 等人的编码层启发。（2016 年）。基本上，它学习视觉中心和平滑因子来创建一个嵌入，同时考虑到上下文信息，同时突出显示依赖于类的特征图。在该模块之上，上下文信息的比例因子通过特征图注意力层（全连接层）来学习。同时，语义编码损失(SE-Loss) 对应于二元交叉熵损失，通过检测对象类别的存在来规范模块的训练（与像素损失不同）。上下文编码模块的输出通过扩张卷积策略进行重塑和处理，同时最小化两个 SE 损失和最终的像素损失。最好的 EncNet 在 PASCAL-Context 挑战中达到了 52.6% 的 mIoU 和 81.2% 的 pixAcc 分数。它还在 2012 年 PASCAL VOC 分割挑战中获得了 85.9% 的 mIoU 分数。

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216165040453-480911002.png)

空洞卷积策略。蓝色为卷积滤波器，D 为膨胀率。在第三和第四阶段之后应用 SE 损失（语义编码损失）来检测对象类别。应用最终的 Seg-loss（逐像素损失）来改进分割。资料来源：H. Zhang 等人。(2018)

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220216165055407-235907579.png)

EncNet 的架构。特征提取器生成特征图作为上下文编码模块的输入。该模块使用语义编码损失进行正则化训练。模块的输出通过扩张卷积策略处理以产生最终分割。资料来源：[ H. Zhang et al. (2018)



---

## 总结

图像语义分割是端到端深度神经网络最近面临的挑战。所有架构之间的主要问题之一是考虑输入的全局视觉上下文以改进分割的预测。最先进的模型使用架构试图链接图像的不同部分，以了解对象之间的关系。

模型在 2012 PASCAL VOC 数据集 (mIoU)、PASCAL-Context 数据集 (mIoU)、2016 / 2017 COCO 数据集 (AP 和 AR) 和 Cityscapes 数据集 (mIoU) 上的得分概览
对整个图像的逐像素预测允许以高精度更好地理解环境。场景理解也可以通过关键点检测、动作识别、视频字幕或视觉问答来实现。在我看来，分割任务与使用多任务损失的这些其他问题相结合，应该有助于超越对场景的全局上下文理解。
最后，我要感谢Long Do Cao帮助我完成所有的帖子，如果你正在寻找一位优秀的高级数据科学家，你应该查看他的个人资料；）。
¹：扩张卷积层已由 [F. Yu 和 V. Koltun (2015)]( https://arxiv.org/pdf/1511.07122.pdf )。它是一个带有扩展过滤器的卷积层（过滤器的神经元不再并排）。扩张率在像素方面固定了两个神经元之间的差距。DeepLab 部分提供了更多详细信息。
²：物体检测、物体分割和关键点检测。
³：Mask R-CNN 模型为预测类别的对象计算二进制掩码（实例优先策略），而不是将每个像素分类为一个类别（分割优先策略）。