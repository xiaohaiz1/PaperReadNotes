边缘检测
1.快速Haar变换
2.快速傅里叶变换
3.Sobel算子
4.Canny算子



### YOLO(You Only Look Once)

构建YOLO3深度学习算法的基础网络架构是**卷积神经网络**(Convolutional Neural Network，简记为CNN)，YOLO是受生物自然视觉认知机制启发而来，当输入感知时，大脑使用不同的皮质区域来抽象不同层次的特征。

视觉系统的处理流程如下图所示。

![image-20210506173834765](/home/linxu/.config/Typora/typora-user-images/image-20210506173834765.png)

人眼感知画面的的过程：
(1) 从原始信号开始，瞳孔摄入像素(pixels);
(2) 初步处理，大脑皮层某些细胞发现边缘和方向(edges);
(3) 抽象，大脑判定眼前的物体的形状部分(object parts);
(4) 进一步抽象，大脑进一步判定该目标内容(object models).



### CNN

CNN的现代结构：由<u>卷积层</u>(Convolutional layers)、<u>池化层</u>(Pooling layers)和<u>全连接层</u>(fully connected layers)三部分组成。

#### 卷积层

卷积层
　　卷积层是 CNN 网络架构中最重要的结构,其主要特性是**局部感知**和**权重共享**。卷积层通过卷积操作对输入信号进行局部特征提取,然后通过激活函数赋予特征非线性学习能力。
在卷积层中的计算一般分为三部分:**卷积运算**、**批量归一化**(BatchNormalization,BN)、**激活函数**(activation function)。

①**卷积运算**
　　卷积运算实际是分析数学中的一种运算方式，在卷积神经网络中通常是仅涉及离散卷积的情形。
卷积运算通过对输入信号的局部进行加权求和，具有<u>线性、平移不变性</u>的特点。

![image-20210506175205317](/home/linxu/.config/Typora/typora-user-images/image-20210506175205317.png)

　　如图 3.2 所示,右侧为 5×5 的输入图像矩阵(输入数据),其对应的卷积核(亦称卷积参数,convolutional kernel 或 convolutional filter)为一个3×3 的矩阵(黄色区域红色字体)。
　　如果假定卷积操作时每做一次卷积,卷积核移动一个像素位置,称移动步长为卷积步长stride=1。
卷积操作从图像 (0,0) 像素开始,通过卷积核中参数与对应位置图像像素逐位相乘,然后累加作为一次卷积操作结
果。
卷积核按照步长大小,在输入图像上自上而下从左至右依次进行卷积操作,最终输出一个 3×3 大小的特征图,随后,该结果也将作为下一层操作的输入。

为了能够提取图像中更多的特征，通常会设置多个卷积核。
假设有$k$个$3×3$的卷积核，按照上述的卷积操作最终可以获得$k$张$3×3$的特征图。
故卷积操作过程的数学表达式可以描述为：
$$
x_j^l = \sum \limits_{i\in M_j} x_i^{l-1}×k_{i,j}^l + b_j^l
$$
其中，$x_j^l$是第$l$层第$j$个通道(如彩色图像为R、G、B三通道图像)的特征图，$M_j$为输入层的通道数，$k_{i,j}^l$代表$l-1$层第$i$个通道特征图对应$l$层第$j$个通道特征图的卷积核，$b$是偏置项。

在最近的研究中,除了经典的卷积操作,还有空洞卷积、转置卷积、可分离卷积等,这些卷积操作在特定的环境下都能发挥很好的作用。

上所述的卷积过程中,输入经过卷积操作后,尺寸都有减小。在深度神经网络中,多次卷积操作使得特征图尺寸变得很小,不利于构建深度网络。同时,对于角落边缘的像素点,只被一个过滤器输出所触碰或者使用。而中间的像素点,卷积核在滑动过程中可多次与之重叠。所以在角落或者边缘区域的像素点在输出中采用较少,这意味着丢掉了图像边缘位置的许多信息。于是实际使用过程中,提出一种padding 操作,通过在输入四周进行数据填充,以此保证输入、输出在卷积前后尺寸不变。

最常见的是零填充,如图 3.3 所示。

<img src="/home/linxu/.config/Typora/typora-user-images/image-20210506191258812.png" alt="image-20210506191258812" style="zoom:67%;" />



②**批量归一化**

　　**Batch Normalization**(BN)是由 Ioffe S[54]于 2015 年所提出用于数据预处理优化的方法。由于训练一个神经网络的本质是学习训练数据的分布,如果每次输入的训练数据分布不同,网络在训练过程中需要去适应不同的分布,从而降低网络的学习速度,并且会降低网络的泛化能力。而采用 BN 之后,使得每个卷积层的输入数据
分布相同,网络的学习速率和泛化能力都得到了加强。

　　BN 结构一般被放置在卷积操作和激活函数之间，让数据在进入激活函数之前做归一化，可以更好利用激活函数在不同区间实现不同的非线性效果。例如 sigmoid函数中间部分是线性区,两端则是饱和非线性区，通过 BN 操作可以让数据分布和激活函数的分布相统一，从而更好地获得非线性表征能力。此外，BN 操作还可以有效防止梯度爆炸和梯度消失的出现。梯度爆炸和梯度消失的原因在于激活函数在一些区间上的梯度很大或者很小，当训练的网络较深时，由于链式法则的传递，导致一些层的梯度很大至无穷大或者很小至零，而采用 BN 之后可以有效避免此类情况。


BN的实现过程，如下表所示

输入：一个batch的训练数据:$Batch= \left\{x_1,x_2,...,x_{m-1},x_m\right\}$，参数:尺度因子$\gamma$,平移因子$\beta$.
过程：

1. 计算这一批数据的均值$\mu_{Batch} = \frac{1}{m} \sum \limits_{i=1}^m X_i$.
2.   计算这一批数据的方差$\sigma_{Batch}^2 = \frac{1}{m} \sum \limits_{i=1}^m (x_i-\mu_{Batch})^2$.
3. 对数据做标准化$\hat X_i = \frac{X_i-\mu_{Batch}}{\sqrt{\sigma_{Batch}^2 + \varepsilon}}$，其中$\varepsilon$是为了避免除数为0时所使用的微小正数
4. 训练参数$$\gamma,\beta$$，计算线性变换结果$y_i =BN_{\gamma,\beta}(x_i) = \gamma \hat x_i + \beta$.

输出：

$$
y_i = \left\{BN_{\gamma,\beta}(x_i) \right\}
$$
③**激活函数**
　　卷积和批量归一化所做操作都是属于线性操作，导致输出特征是线性的。
而样本中的特征大多都是非线性相关,需要通过激活函数赋予该层对于非线性变换的能力。多层卷积层的级联实质上是非线性变换的级联,可以有效提升网络对于特征基于 YOLOv3 深度学习增强的目标识别算法学习的能力。在 CNN 网络中**线性整流函数**(Relu)常被用于激活函数,这是因为Relu 激活函数在网络训练过程中可以更快收敛,加快训练速度,且在输入为正数时,防止产生梯度饱和。其余激活函数及特点详见表 3.2。

**常见激活函数及介绍**

![image-20210506193059360](/home/linxu/.config/Typora/typora-user-images/image-20210506193059360.png)

**池化层**

　　在卷积神经网络中池化层也称为池化采样,常常位于卷积层后面,池化采样操作主要用于减少特征图的大小以及网络的参数量。

　　常见的池化操作主要有两种:**最大值池化**(max pooling),**平均值池化**(mean pooling)。
与卷积层不同的是，池化层不包含需要学习的参数，仅需指定池化类型(average 或 max 等)、池化操作的核大小(kernel size)和池化操作的步长(stride)。


　　






